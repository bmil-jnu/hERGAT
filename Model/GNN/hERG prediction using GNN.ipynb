{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fd2a4b9-86bb-4701-98f7-bf96609e18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9666da3-3456-4d8c-a548-abcb1d2b6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "task='classification'  # target is a binary value (e.g., drug or not).\n",
    "dataset='hiv'\n",
    "\n",
    "\n",
    "radius=2\n",
    "dim=50\n",
    "layer_hidden=10\n",
    "layer_output=10\n",
    "\n",
    "batch_train=32\n",
    "batch_test=32\n",
    "lr=1e-4\n",
    "lr_decay=0.99\n",
    "decay_interval=10\n",
    "iteration=300\n",
    "\n",
    "setting= '$dataset--radius$radius--dim$dim--layer_hidden$layer_hidden--layer_output$layer_output--batch_train$batch_train--batch_test$batch_test--lr$lr--lr_decay$lr_decay--decay_interval$decay_interval--iteration$iteration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00dd11d-1ae5-4b21-ba37-c2c61f263c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_atoms(mol, atom_dict):\n",
    "    \"\"\"Transform the atom types in a molecule (e.g., H, C, and O)\n",
    "    into the indices (e.g., H=0, C=1, and O=2).\n",
    "    Note that each atom index considers the aromaticity.\n",
    "    \"\"\"\n",
    "    atoms = [a.GetSymbol() for a in mol.GetAtoms()]\n",
    "    for a in mol.GetAromaticAtoms():\n",
    "        i = a.GetIdx()\n",
    "        atoms[i] = (atoms[i], 'aromatic')\n",
    "    atoms = [atom_dict[a] for a in atoms]\n",
    "    return np.array(atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e011c783-9ed3-4797-9bc5-1a720db8cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ijbonddict(mol, bond_dict):\n",
    "    \"\"\"Create a dictionary, in which each key is a node ID\n",
    "    and each value is the tuples of its neighboring node\n",
    "    and chemical bond (e.g., single and double) IDs.\n",
    "    \"\"\"\n",
    "    i_jbond_dict = defaultdict(lambda: [])\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bond = bond_dict[str(b.GetBondType())]\n",
    "        i_jbond_dict[i].append((j, bond))\n",
    "        i_jbond_dict[j].append((i, bond))\n",
    "    return i_jbond_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab8e9f4-90d1-40b7-bed6-7d36ad7048fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fingerprints(radius, atoms, i_jbond_dict,\n",
    "                         fingerprint_dict, edge_dict):\n",
    "    \"\"\"Extract the fingerprints from a molecular graph\n",
    "    based on Weisfeiler-Lehman algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    if (len(atoms) == 1) or (radius == 0):\n",
    "        nodes = [fingerprint_dict[a] for a in atoms]\n",
    "\n",
    "    else:\n",
    "        nodes = atoms\n",
    "        i_jedge_dict = i_jbond_dict\n",
    "\n",
    "        for _ in range(radius):\n",
    "\n",
    "            \"\"\"Update each node ID considering its neighboring nodes and edges.\n",
    "            The updated node IDs are the fingerprint IDs.\n",
    "            \"\"\"\n",
    "            nodes_ = []\n",
    "            for i, j_edge in i_jedge_dict.items():\n",
    "                neighbors = [(nodes[j], edge) for j, edge in j_edge]\n",
    "                fingerprint = (nodes[i], tuple(sorted(neighbors)))\n",
    "                nodes_.append(fingerprint_dict[fingerprint])\n",
    "\n",
    "            \"\"\"Also update each edge ID considering\n",
    "            its two nodes on both sides.\n",
    "            \"\"\"\n",
    "            i_jedge_dict_ = defaultdict(lambda: [])\n",
    "            for i, j_edge in i_jedge_dict.items():\n",
    "                for j, edge in j_edge:\n",
    "                    both_side = tuple(sorted((nodes[i], nodes[j])))\n",
    "                    edge = edge_dict[(both_side, edge)]\n",
    "                    i_jedge_dict_[i].append((j, edge))\n",
    "\n",
    "            nodes = nodes_\n",
    "            i_jedge_dict = i_jedge_dict_\n",
    "\n",
    "    return np.array(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c14391-1c03-4908-b7af-883ee9da8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, ratio):\n",
    "    \"\"\"Shuffle and split a dataset.\"\"\"\n",
    "    np.random.seed(1234)  # fix the seed for shuffle.\n",
    "    np.random.shuffle(dataset)\n",
    "    n = int(ratio * len(dataset))\n",
    "    return dataset[:n], dataset[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0b24fc8-055c-49d9-890c-2a31d46c8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(task, dataset, radius, device):\n",
    "\n",
    "    dir_dataset = ''\n",
    "\n",
    "    \"\"\"Initialize x_dict, in which each key is a symbol type\n",
    "    (e.g., atom and chemical bond) and each value is its index.\n",
    "    \"\"\"\n",
    "    atom_dict = defaultdict(lambda: len(atom_dict))\n",
    "    bond_dict = defaultdict(lambda: len(bond_dict))\n",
    "    fingerprint_dict = defaultdict(lambda: len(fingerprint_dict))\n",
    "    edge_dict = defaultdict(lambda: len(edge_dict))\n",
    "\n",
    "    def create_dataset(filename):\n",
    "\n",
    "        print(filename)\n",
    "\n",
    "        \"\"\"Load a dataset.\"\"\"\n",
    "        with open(dir_dataset + filename, 'r') as f:\n",
    "            smiles_property = f.readline().strip().split(' ')\n",
    "            data_original = f.read().strip().split('\\n')\n",
    "\n",
    "        \"\"\"Exclude the data contains '.' in its smiles.\"\"\"\n",
    "        data_original = [data for data in data_original\n",
    "                         if '.' not in data.split()[0]]\n",
    "\n",
    "        dataset = []\n",
    "\n",
    "        for data in data_original:\n",
    "\n",
    "            smiles, property = data.strip().split()\n",
    "\n",
    "            \"\"\"Create each data with the above defined functions.\"\"\"\n",
    "            mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "            atoms = create_atoms(mol, atom_dict)\n",
    "            molecular_size = len(atoms)\n",
    "            i_jbond_dict = create_ijbonddict(mol, bond_dict)\n",
    "            fingerprints = extract_fingerprints(radius, atoms, i_jbond_dict,\n",
    "                                                fingerprint_dict, edge_dict)\n",
    "            adjacency = Chem.GetAdjacencyMatrix(mol)\n",
    "            np.fill_diagonal(adjacency, 1)\n",
    "            \n",
    "            \n",
    "            \"\"\"Transform the above each data of numpy\n",
    "            to pytorch tensor on a device (i.e., CPU or GPU).\n",
    "            \"\"\"\n",
    "            fingerprints = torch.LongTensor(fingerprints).to(device)\n",
    "            adjacency = torch.FloatTensor(adjacency).to(device)\n",
    "            if task == 'classification':\n",
    "                property = torch.LongTensor([int(property)]).to(device)\n",
    "            if task == 'regression':\n",
    "                property = torch.FloatTensor([[float(property)]]).to(device)\n",
    "\n",
    "            dataset.append((fingerprints, adjacency, molecular_size, property))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    dataset_train = create_dataset('train_data.txt')\n",
    "    dataset_train, dataset_dev = split_dataset(dataset_train, 0.9)\n",
    "    dataset_test = create_dataset('CardioTox_dataset.txt')\n",
    "\n",
    "    N_fingerprints = len(fingerprint_dict)\n",
    "\n",
    "    return dataset_train, dataset_dev, dataset_test, N_fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f86284ea-6d6c-4827-bc61-f501dab4d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(dataset_train)):\n",
    "#    torch.diagonal(dataset_train[i][1]).fill_(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7949964b-1d35-4543-b002-28c6528a41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "beb587dc-1bf9-44f3-8eb0-173d23fb945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c63fc9c-980f-414e-923b-45c5de21de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularGraphNeuralNetwork(nn.Module):\n",
    "    def __init__(self, N_fingerprints, dim, layer_hidden, layer_output):\n",
    "        super(MolecularGraphNeuralNetwork, self).__init__()\n",
    "        self.embed_fingerprint = nn.Embedding(N_fingerprints, dim)\n",
    "        self.W_fingerprint = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                            for _ in range(layer_hidden)])\n",
    "        self.W_output = nn.ModuleList([nn.Linear(dim, dim)\n",
    "                                       for _ in range(layer_output)])\n",
    "        if task == 'classification':\n",
    "            self.W_property = nn.Linear(dim, 2)\n",
    "        if task == 'regression':\n",
    "            self.W_property = nn.Linear(dim, 1)\n",
    "\n",
    "    def pad(self, matrices, pad_value):\n",
    "        \"\"\"Pad the list of matrices\n",
    "        with a pad_value (e.g., 0) for batch processing.\n",
    "        For example, given a list of matrices [A, B, C],\n",
    "        we obtain a new matrix [A00, 0B0, 00C],\n",
    "        where 0 is the zero (i.e., pad value) matrix.\n",
    "        \"\"\"\n",
    "        shapes = [m.shape for m in matrices]\n",
    "        M, N = sum([s[0] for s in shapes]), sum([s[1] for s in shapes])\n",
    "        zeros = torch.FloatTensor(np.zeros((M, N))).to(device)\n",
    "        pad_matrices = pad_value + zeros\n",
    "        i, j = 0, 0\n",
    "        for k, matrix in enumerate(matrices):\n",
    "            m, n = shapes[k]\n",
    "            pad_matrices[i:i+m, j:j+n] = matrix\n",
    "            i += m\n",
    "            j += n\n",
    "        return pad_matrices\n",
    "\n",
    "    def update(self, matrix, vectors, layer):\n",
    "        hidden_vectors = torch.relu(self.W_fingerprint[layer](vectors))\n",
    "        return hidden_vectors + torch.matmul(matrix, hidden_vectors)\n",
    "\n",
    "    def sum(self, vectors, axis):\n",
    "        sum_vectors = [torch.sum(v, 0) for v in torch.split(vectors, axis)]\n",
    "        return torch.stack(sum_vectors)\n",
    "\n",
    "    def mean(self, vectors, axis):\n",
    "        mean_vectors = [torch.mean(v, 0) for v in torch.split(vectors, axis)]\n",
    "        return torch.stack(mean_vectors)\n",
    "\n",
    "    def gnn(self, inputs):\n",
    "\n",
    "        \"\"\"Cat or pad each input data for batch processing.\"\"\"\n",
    "        fingerprints, adjacencies, molecular_sizes = inputs\n",
    "        fingerprints = torch.cat(fingerprints)\n",
    "        adjacencies = self.pad(adjacencies, 0)\n",
    "\n",
    "        \"\"\"GNN layer (update the fingerprint vectors).\"\"\"\n",
    "        fingerprint_vectors = self.embed_fingerprint(fingerprints)\n",
    "        for l in range(layer_hidden):\n",
    "            hs = self.update(adjacencies, fingerprint_vectors, l)\n",
    "            fingerprint_vectors = F.normalize(hs, 2, 1)  # normalize.\n",
    "\n",
    "        \"\"\"Molecular vector by sum or mean of the fingerprint vectors.\"\"\"\n",
    "        molecular_vectors = self.sum(fingerprint_vectors, molecular_sizes)\n",
    "        # molecular_vectors = self.mean(fingerprint_vectors, molecular_sizes)\n",
    "\n",
    "        return molecular_vectors\n",
    "\n",
    "    def mlp(self, vectors):\n",
    "        \"\"\"Classifier or regressor based on multilayer perceptron.\"\"\"\n",
    "        for l in range(layer_output):\n",
    "            vectors = torch.relu(self.W_output[l](vectors))\n",
    "        outputs = self.W_property(vectors)\n",
    "        return outputs\n",
    "\n",
    "    def forward_classifier(self, data_batch, train):\n",
    "\n",
    "        inputs = data_batch[:-1]\n",
    "        correct_labels = torch.cat(data_batch[-1])\n",
    "\n",
    "        if train:\n",
    "            molecular_vectors = self.gnn(inputs)\n",
    "            predicted_scores = self.mlp(molecular_vectors)\n",
    "            loss = F.cross_entropy(predicted_scores, correct_labels)\n",
    "            return loss\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                molecular_vectors = self.gnn(inputs)\n",
    "                predicted_scores = self.mlp(molecular_vectors)\n",
    "            predicted_scores = predicted_scores.to('cpu').data.numpy()\n",
    "            predicted_scores = [s[1] for s in predicted_scores]\n",
    "            correct_labels = correct_labels.to('cpu').data.numpy()\n",
    "            return predicted_scores, correct_labels\n",
    "\n",
    "    def forward_regressor(self, data_batch, train):\n",
    "\n",
    "        inputs = data_batch[:-1]\n",
    "        correct_values = torch.cat(data_batch[-1])\n",
    "\n",
    "        if train:\n",
    "            molecular_vectors = self.gnn(inputs)\n",
    "            predicted_values = self.mlp(molecular_vectors)\n",
    "            loss = F.mse_loss(predicted_values, correct_values)\n",
    "            return loss\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                molecular_vectors = self.gnn(inputs)\n",
    "                predicted_values = self.mlp(molecular_vectors)\n",
    "            predicted_values = predicted_values.to('cpu').data.numpy()\n",
    "            correct_values = correct_values.to('cpu').data.numpy()\n",
    "            predicted_values = np.concatenate(predicted_values)\n",
    "            correct_values = np.concatenate(correct_values)\n",
    "            return predicted_values, correct_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01d15d78-afff-4e8c-a90c-29878f97a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(self, dataset):\n",
    "        np.random.shuffle(dataset)\n",
    "        N = len(dataset)\n",
    "        loss_total = 0\n",
    "        for i in range(0, N, batch_train):\n",
    "            data_batch = list(zip(*dataset[i:i+batch_train]))\n",
    "            if task == 'classification':\n",
    "                loss = self.model.forward_classifier(data_batch, train=True)\n",
    "            if task == 'regression':\n",
    "                loss = self.model.forward_regressor(data_batch, train=True)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.item()\n",
    "        return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cc8f48c-1abe-48ab-be9a-13fb2164786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_values(A):\n",
    "    for i in range(len(A)):\n",
    "        if isinstance(A[i], list):\n",
    "            replace_values(A[i])\n",
    "        elif A[i] > 0.5:\n",
    "            A[i] = 1\n",
    "        else:\n",
    "            A[i] = 0\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e66bda4e-1658-41cb-8644-add38a35acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "\n",
    "    def test_classifier(self, dataset):\n",
    "        N = len(dataset)\n",
    "        P, C = [], []\n",
    "        for i in range(0, N, batch_test):\n",
    "            data_batch = list(zip(*dataset[i:i+batch_test]))\n",
    "            predicted_scores, correct_labels = self.model.forward_classifier(\n",
    "                                               data_batch, train=False)\n",
    "            P.append(predicted_scores)\n",
    "            C.append(correct_labels)\n",
    "            A = replace_values(P)\n",
    "        AUC = roc_auc_score(np.concatenate(C), np.concatenate(P))\n",
    "        acc = accuracy_score(np.concatenate(C), np.concatenate(A))\n",
    "        precision = precision_score(np.concatenate(C), np.concatenate(P))\n",
    "        recall = recall_score(np.concatenate(C), np.concatenate(P))\n",
    "        \n",
    "        pr_precision, pr_recall, _ = precision_recall_curve(np.concatenate(C), np.concatenate(P))\n",
    "        aupr = auc(pr_recall, pr_precision)\n",
    "\n",
    "        f1 = f1_score(np.concatenate(C), np.concatenate(P))\n",
    "        return AUC, acc, precision, recall, f1, aupr\n",
    "\n",
    "    def test_regressor(self, dataset):\n",
    "        N = len(dataset)\n",
    "        SAE = 0  # sum absolute error.\n",
    "        for i in range(0, N, batch_test):\n",
    "            data_batch = list(zip(*dataset[i:i+batch_test]))\n",
    "            predicted_values, correct_values = self.model.forward_regressor(\n",
    "                                               data_batch, train=False)\n",
    "            SAE += sum(np.abs(predicted_values-correct_values))\n",
    "        MAE = SAE / N  # mean absolute error.\n",
    "        return MAE\n",
    "\n",
    "    def save_result(self, result, filename):\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write(result + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d13c2b5-0062-4433-b8d4-aa3ba9d105d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code uses a GPU!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Preprocessing the hiv dataset.\n",
      "Just a moment......\n",
      "train_data.txt\n",
      "CardioTox_dataset.txt\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The preprocess has finished!\n",
      "# of training data samples: 5708\n",
      "# of development data samples: 635\n",
      "# of test data samples: 1219\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Creating a model.\n",
      "# of model parameters: 460452\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/dohyeon/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training will finish in about 0 hours 38 minutes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1\t7.751988395117223\t118.93354332447052\t0.5\t0.4807219031993437\t0.0\t0.0\t0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/dohyeon/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t17.40829761326313\t116.51892626285553\t0.5\t0.4807219031993437\t0.0\t0.0\t0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/bmi-lab/anaconda3/envs/dohyeon/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t28.654764148406684\t116.39661568403244\t0.5\t0.4807219031993437\t0.0\t0.0\t0.0\n",
      "4\t40.69194746017456\t112.45038574934006\t0.5956574953226684\t0.5857260049220673\t0.7133333333333334\t0.3380726698262243\t0.4587352625937835\n",
      "5\t52.0922018494457\t105.95346635580063\t0.6527667696488363\t0.6513535684987695\t0.6818181818181818\t0.6161137440758294\t0.6473029045643154\n",
      "6\t63.79742518812418\t101.5245410501957\t0.6675293445265786\t0.6636587366694011\t0.7252525252525253\t0.5671406003159558\t0.6365248226950355\n",
      "7\t78.7046517888084\t98.74435639381409\t0.643274617321493\t0.6341263330598852\t0.7859327217125383\t0.4060031595576619\t0.5354166666666668\n",
      "8\t88.0229255631566\t96.20314860343933\t0.6797955453472009\t0.6735028712059065\t0.7804295942720764\t0.5165876777251185\t0.6216730038022813\n",
      "9\t95.12005481962115\t94.07470744848251\t0.6827946988445508\t0.6776045939294504\t0.76431718061674\t0.5481832543443917\t0.6384544618215271\n",
      "10\t102.37020887248218\t91.43920156359673\t0.678680803800096\t0.6759639048400328\t0.7236842105263158\t0.608214849921011\t0.6609442060085836\n",
      "11\t109.3375286841765\t90.44307175278664\t0.6827650442931164\t0.6767842493847416\t0.7785547785547785\t0.5276461295418642\t0.6290018832391714\n",
      "12\t119.09351017698646\t90.74485874176025\t0.6836142428114671\t0.6792452830188679\t0.7520833333333333\t0.5703001579778831\t0.6486972147349506\n",
      "13\t125.88814815599471\t87.10633063316345\t0.6742070103359592\t0.6661197703035275\t0.8121546961325967\t0.46445497630331756\t0.5909547738693468\n",
      "14\t132.769064437598\t87.52223348617554\t0.6649817489715262\t0.6562756357670222\t0.8128654970760234\t0.4391785150078989\t0.5702564102564103\n",
      "15\t140.21408022753894\t84.28476616740227\t0.7003919792525974\t0.6964725184577523\t0.7656565656565657\t0.5987361769352291\t0.6719858156028369\n",
      "16\t150.44800131302327\t82.19366629421711\t0.656136605039117\t0.6464315012305168\t0.8258064516129032\t0.40442338072669826\t0.542948038176034\n",
      "17\t160.33050582464784\t80.11743222177029\t0.7303740786870041\t0.7284659557013946\t0.7696428571428572\t0.6808846761453397\t0.7225481978206203\n",
      "18\t170.0474954508245\t78.4446192085743\t0.7050720066426195\t0.6997538966365874\t0.7960088691796009\t0.5671406003159558\t0.6623616236162363\n",
      "19\t181.3440137580037\t77.49304547905922\t0.7192859723188242\t0.716160787530763\t0.7754318618042226\t0.6382306477093207\t0.7001733102253033\n",
      "20\t194.3405756475404\t76.63041055202484\t0.7005227288657404\t0.6948318293683347\t0.7972665148063781\t0.5529225908372828\t0.6529850746268657\n",
      "21\t206.8235844168812\t73.40560771524906\t0.6867481897244283\t0.6800656275635767\t0.7985257985257985\t0.5134281200631912\t0.625\n",
      "22\t217.75879191607237\t73.45711547136307\t0.7335969892542689\t0.7317473338802297\t0.7722419928825622\t0.6856240126382307\t0.7263598326359834\n",
      "23\t228.45197508204728\t71.81717267632484\t0.729111064382727\t0.726825266611977\t0.7737226277372263\t0.669826224328594\t0.7180355630821337\n",
      "24\t236.0624692644924\t70.60852904617786\t0.738902458092727\t0.7383100902379\t0.760797342192691\t0.7235387045813586\t0.7417004048582996\n",
      "25\t247.41306819580495\t69.80819326639175\t0.7135707314969079\t0.7087776866283839\t0.7970085470085471\t0.5892575039494471\t0.6775658492279746\n",
      "26\t257.3479487374425\t67.87130562961102\t0.6976165828251621\t0.6915504511894995\t0.8009367681498829\t0.5402843601895735\t0.6452830188679246\n",
      "27\t266.7835296494886\t65.824601739645\t0.7384333770064\t0.7358490566037735\t0.7884972170686456\t0.6714060031595577\t0.7252559726962458\n",
      "28\t276.05063726752996\t65.84122072160244\t0.7293348214526416\t0.7260049220672683\t0.7902912621359224\t0.6429699842022117\t0.7090592334494774\n",
      "29\t286.60057778377086\t63.967331290245056\t0.7412761701416409\t0.7391304347826086\t0.7848101265822784\t0.6856240126382307\t0.7318718381112985\n",
      "30\t300.2160154171288\t61.96538907289505\t0.7433587284128345\t0.7424118129614438\t0.7698815566835872\t0.7187993680884676\t0.7434640522875817\n",
      "31\t312.0401099929586\t61.670464500784874\t0.7525880335797357\t0.7506152584085316\t0.7942754919499105\t0.7014218009478673\t0.7449664429530201\n",
      "32\t324.3510302323848\t60.97073772549629\t0.7484781823377491\t0.7473338802296965\t0.7777777777777778\t0.7187993680884676\t0.7471264367816092\n",
      "33\t336.154842752032\t60.18567717075348\t0.7385304282656402\t0.7350287120590648\t0.803921568627451\t0.6477093206951027\t0.7174103237095364\n",
      "34\t342.3829023996368\t58.770800337195396\t0.7600394135947247\t0.760459392945037\t0.768503937007874\t0.7709320695102686\t0.7697160883280757\n",
      "35\t350.53936816658825\t57.2113514021039\t0.7388431489898581\t0.7366694011484823\t0.782608695652174\t0.6824644549763034\t0.7291139240506329\n",
      "36\t360.28511039353907\t56.539375834167004\t0.7571373113566148\t0.7555373256767842\t0.7933450087565674\t0.7156398104265402\t0.7524916943521595\n",
      "37\t370.86357806809247\t55.76808684319258\t0.7424758315405809\t0.7407711238720263\t0.7795414462081128\t0.69826224328594\t0.7366666666666668\n",
      "38\t381.03112417832017\t56.65767111629248\t0.7506874464196173\t0.7506152584085316\t0.7657512116316639\t0.7488151658767772\t0.7571884984025558\n",
      "39\t393.65930945333093\t55.05323288589716\t0.7619319131499065\t0.7637407711238721\t0.7540500736377025\t0.8088467614533965\t0.7804878048780487\n",
      "40\t406.1924209240824\t53.450290732085705\t0.7593762299899175\t0.7596390484003281\t0.7698412698412699\t0.7661927330173776\t0.768012668250198\n",
      "41\t417.9521399019286\t52.668409302830696\t0.7606729426480975\t0.760459392945037\t0.7772357723577236\t0.7551342812006319\t0.7660256410256411\n",
      "42\t433.5302741928026\t51.74826016277075\t0.7551437167397247\t0.7547169811320755\t0.774671052631579\t0.7440758293838863\t0.7590652699435939\n",
      "43\t446.3720118617639\t51.186882205307484\t0.7546962025998954\t0.756357670221493\t0.7492581602373887\t0.7977883096366508\t0.772762050497322\n",
      "44\t459.25730698835105\t50.15331396088004\t0.74762494001693\t0.7465135356849877\t0.7764505119453925\t0.7187993680884676\t0.7465135356849876\n",
      "45\t472.77042786218226\t49.99417196214199\t0.758362583504521\t0.7596390484003281\t0.756797583081571\t0.7914691943127962\t0.7737451737451737\n",
      "46\t486.704720467329\t47.699979320168495\t0.7542311653160366\t0.7522559474979491\t0.7960644007155635\t0.7030015797788309\t0.7466442953020134\n",
      "47\t498.50851179100573\t47.508310835808516\t0.7615854940717854\t0.7629204265791633\t0.7590361445783133\t0.7962085308056872\t0.7771781033153432\n",
      "48\t510.6974080009386\t46.85146038979292\t0.7621556702198212\t0.7629204265791633\t0.7662538699690402\t0.7819905213270142\t0.7740422204847537\n",
      "49\t521.5054069878533\t47.174012057483196\t0.759562244903461\t0.7612797374897456\t0.7529585798816568\t0.8041074249605056\t0.7776928953399541\n",
      "50\t533.2434499403462\t46.08741553127766\t0.7664178380214482\t0.7686628383921247\t0.7532467532467533\t0.8246445497630331\t0.7873303167420814\n",
      "51\t544.5197253962979\t45.278109323233366\t0.7594988919981236\t0.7612797374897456\t0.7522123893805309\t0.8056872037914692\t0.7780320366132722\n",
      "52\t554.6202061343938\t44.044247306883335\t0.767241425790833\t0.7686628383921247\t0.7631184407796102\t0.8041074249605056\t0.7830769230769231\n",
      "53\t564.1565941562876\t44.17449790984392\t0.7657586982191094\t0.7662018047579984\t0.7735849056603774\t0.7772511848341233\t0.7754137115839244\n",
      "54\t577.4464439461008\t42.8717295601964\t0.7416562875736645\t0.7391304347826086\t0.7911275415896488\t0.6761453396524486\t0.7291311754684838\n",
      "55\t590.9807232338935\t42.232269410043955\t0.761745898236363\t0.7621000820344545\t0.7709320695102686\t0.7709320695102686\t0.7709320695102686\n",
      "56\t601.3200755855069\t41.232283141463995\t0.7668316538073748\t0.7678424938474159\t0.7675840978593272\t0.7930489731437599\t0.78010878010878\n",
      "57\t615.61034556292\t40.948191590607166\t0.7564741277518076\t0.7547169811320755\t0.7950530035335689\t0.7109004739336493\t0.7506255212677232\n",
      "58\t629.1435888037086\t39.02154388651252\t0.7597226490680383\t0.760459392945037\t0.7643410852713178\t0.7788309636650869\t0.7715179968701096\n",
      "59\t640.6859019696712\t40.43346696719527\t0.73903320770587\t0.7366694011484823\t0.7857142857142857\t0.6777251184834123\t0.727735368956743\n",
      "60\t650.7451861593872\t39.4006931707263\t0.7652855733303141\t0.7653814602132896\t0.7776\t0.7677725118483413\t0.7726550079491257\n",
      "61\t659.7395561011508\t37.586101315915585\t0.7561236648712184\t0.7555373256767842\t0.7777777777777778\t0.740916271721959\t0.7588996763754046\n",
      "62\t672.3555627772585\t38.70592214539647\t0.7606648550431608\t0.7637407711238721\t0.7399165507649513\t0.8404423380726699\t0.7869822485207101\n",
      "63\t682.5076384628192\t38.0131286457181\t0.7696407485887129\t0.7719442165709598\t0.7553956834532374\t0.8293838862559242\t0.7906626506024096\n",
      "64\t694.8005517870188\t36.93770384788513\t0.763486081231904\t0.7629204265791633\t0.7847682119205298\t0.7488151658767772\t0.7663702506063056\n",
      "65\t704.437042992562\t37.21018612757325\t0.7617162436849286\t0.7612797374897456\t0.78125\t0.7503949447077409\t0.765511684125705\n",
      "66\t713.3511473378167\t34.499886970967054\t0.7660080660379901\t0.7678424938474159\t0.7573529411764706\t0.8135860979462876\t0.7844630616907845\n",
      "67\t727.0478369789198\t38.33015904575586\t0.7565671352085793\t0.7555373256767842\t0.7843803056027164\t0.7298578199052133\t0.7561374795417348\n",
      "68\t738.2751741977409\t35.148781016469\t0.7547892100566671\t0.7571780147662018\t0.7417503586800573\t0.8167456556082149\t0.7774436090225564\n",
      "69\t749.968590868637\t35.23569815605879\t0.7569432088381347\t0.7571780147662018\t0.7678855325914149\t0.7630331753554502\t0.7654516640253566\n",
      "70\t759.4672094155103\t33.787231143563986\t0.7609560088208811\t0.7612797374897456\t0.7705696202531646\t0.7693522906793049\t0.7699604743083003\n",
      "71\t769.5609093578532\t32.34286117181182\t0.7494284759178084\t0.7473338802296965\t0.7927927927927928\t0.6951026856240127\t0.7407407407407407\n",
      "72\t780.4380595348775\t33.14162078127265\t0.7686311459057847\t0.7703035274815423\t0.7614814814814815\t0.8120063191153238\t0.7859327217125383\n",
      "73\t791.9115358768031\t32.321506103500724\t0.7609263542694467\t0.760459392945037\t0.7808896210873146\t0.7488151658767772\t0.764516129032258\n",
      "74\t801.6615318339318\t33.05693044140935\t0.7631059637998803\t0.7629204265791633\t0.7792207792207793\t0.7582938388625592\t0.7686148919135308\n",
      "75\t809.8007443575189\t30.837137846276164\t0.763198971256652\t0.7637407711238721\t0.7699530516431925\t0.7772511848341233\t0.7735849056603774\n",
      "76\t818.3273148816079\t29.62553185969591\t0.7661684702025675\t0.7670221493027072\t0.7688751926040062\t0.7883096366508688\t0.7784711388455539\n",
      "77\t829.7284682057798\t30.13788465783\t0.7709711596007958\t0.7719442165709598\t0.7718223583460949\t0.7962085308056872\t0.7838258164852254\n",
      "78\t840.4266951102763\t30.774695124477148\t0.752964107209291\t0.7522559474979491\t0.7762938230383973\t0.7345971563981043\t0.75487012987013\n",
      "79\t849.8041190756485\t29.69621242210269\t0.7728636591559777\t0.7752255947497949\t0.757532281205165\t0.8341232227488151\t0.793984962406015\n",
      "80\t859.5674713626504\t28.529145665466785\t0.7689182558810367\t0.7694831829368335\t0.775\t0.7835703001579779\t0.779261586802828\n",
      "81\t868.5400651786476\t29.476110147312284\t0.7652774857253772\t0.7686628383921247\t0.7407407407407407\t0.8530805687203792\t0.7929515418502202\n",
      "82\t879.1160315973684\t28.57358131930232\t0.7755837902830123\t0.7768662838392125\t0.7722473604826546\t0.8088467614533965\t0.7901234567901234\n",
      "83\t888.0258330488577\t28.642024751752615\t0.779216472833735\t0.7809680065627563\t0.7699115044247787\t0.8246445497630331\t0.7963386727688787\n",
      "84\t899.6516445344314\t29.70340943709016\t0.7689775649839056\t0.771123872026251\t0.7565217391304347\t0.8246445497630331\t0.7891156462585034\n",
      "85\t912.0038046659902\t27.296453528106213\t0.7805765383972523\t0.7817883511074651\t0.7776096822995462\t0.8120063191153238\t0.794435857805255\n",
      "86\t922.2813386106864\t27.63973582163453\t0.7709037628929901\t0.7735849056603774\t0.7524752475247525\t0.8404423380726699\t0.7940298507462686\n",
      "87\t930.7198817087337\t26.15082259848714\t0.7655686395030975\t0.7662018047579984\t0.7710280373831776\t0.7819905213270142\t0.7764705882352941\n",
      "88\t940.1707147592679\t26.051846588030457\t0.7722341739050732\t0.7735849056603774\t0.7684210526315789\t0.8072669826224329\t0.7873651771956857\n",
      "89\t952.8063984839246\t24.81296323426068\t0.7613024278990019\t0.7621000820344545\t0.7650695517774343\t0.7819905213270142\t0.7734375\n",
      "90\t965.775851382874\t26.196624482050538\t0.7609897071747838\t0.760459392945037\t0.7818181818181819\t0.7472353870458136\t0.7641357027463651\n",
      "91\t979.4394160881639\t26.638865690678358\t0.7714105861356884\t0.7735849056603774\t0.7583212735166426\t0.8278041074249605\t0.7915407854984894\n",
      "92\t990.1961185783148\t25.439634073525667\t0.7722597846540391\t0.7760459392945037\t0.7425876010781671\t0.8704581358609794\t0.8014545454545454\n",
      "93\t1000.2176550123841\t25.523128358647227\t0.7806695458540241\t0.782608695652174\t0.7690058479532164\t0.8309636650868878\t0.7987851176917237\n",
      "94\t1010.7002572380006\t24.228229640051723\t0.7795965902657587\t0.7809680065627563\t0.7747747747747747\t0.8151658767772512\t0.7944572748267897\n",
      "95\t1021.0962344547734\t23.422829158604145\t0.7674314845068446\t0.7686628383921247\t0.7655068078668684\t0.7993680884676145\t0.7820710973724884\n",
      "96\t1031.428586781025\t23.361120345070958\t0.7672036836344619\t0.771123872026251\t0.7372654155495979\t0.8688783570300158\t0.7976794778825236\n",
      "97\t1042.8817019807175\t27.4909927835688\t0.7661307280461964\t0.7694831829368335\t0.7417582417582418\t0.8530805687203792\t0.7935341660543719\n",
      "98\t1054.829342125915\t25.397039961069822\t0.7742156371145583\t0.7793273174733388\t0.7321428571428571\t0.9067930489731437\t0.8101623147494706\n",
      "99\t1066.846933226101\t23.22087404690683\t0.7787730024963739\t0.7809680065627563\t0.7644508670520231\t0.8357030015797788\t0.7984905660377358\n",
      "100\t1080.0204553306103\t21.383422936312854\t0.7802894284220006\t0.782608695652174\t0.764367816091954\t0.8404423380726699\t0.8006019563581641\n",
      "101\t1090.842866086401\t20.886563395150006\t0.7863591759269744\t0.7875307629204266\t0.7833333333333333\t0.8167456556082149\t0.7996906419180202\n",
      "102\t1101.2874533003196\t22.16356608644128\t0.7798163035332051\t0.7817883511074651\t0.7678832116788321\t0.8309636650868878\t0.7981790591805765\n",
      "103\t1111.1716172499582\t23.412366397678852\t0.784965412009554\t0.7875307629204266\t0.765625\t0.8515007898894155\t0.8062827225130891\n",
      "104\t1123.062226924114\t22.5136504676193\t0.7695517849344097\t0.7694831829368335\t0.7838709677419354\t0.7677725118483413\t0.7757382282521946\n",
      "105\t1133.891438712366\t20.375592421274632\t0.7815820972777122\t0.7850697292863003\t0.7530695770804912\t0.8720379146919431\t0.808199121522694\n",
      "106\t1144.2849248657003\t22.683191239833832\t0.7746038421515186\t0.7760459392945037\t0.7694610778443114\t0.8120063191153238\t0.7901614142966947\n",
      "107\t1152.7329556569457\t21.50298856012523\t0.7716976961109403\t0.7727645611156686\t0.7713414634146342\t0.7993680884676145\t0.7851047323506595\n",
      "108\t1166.1852494813502\t21.721695641987026\t0.7678749548442058\t0.7686628383921247\t0.7712519319938176\t0.7883096366508688\t0.7796875\n",
      "109\t1178.850295796059\t21.570276823826134\t0.7718244019216149\t0.7727645611156686\t0.7730061349693251\t0.7962085308056872\t0.7844357976653696\n",
      "110\t1186.8201014101505\t19.47214726638049\t0.779372833195844\t0.7817883511074651\t0.7625178826895566\t0.8420221169036335\t0.8003003003003004\n",
      "111\t1195.7262247391045\t21.208944668527693\t0.7715669464977976\t0.7744052502050861\t0.7514044943820225\t0.8451816745655608\t0.7955390334572491\n",
      "112\t1205.995947607793\t21.303704285994172\t0.783355978627156\t0.7850697292863003\t0.7740029542097489\t0.8278041074249605\t0.8\n",
      "113\t1214.9389897361398\t17.753097812645137\t0.7740673643573859\t0.7752255947497949\t0.7723823975720789\t0.8041074249605056\t0.7879256965944273\n",
      "114\t1223.9631102867424\t21.692275658249855\t0.776432988801363\t0.7793273174733388\t0.7549019607843137\t0.8515007898894155\t0.8002969561989608\n",
      "115\t1233.9668103251606\t18.32943687075749\t0.7786462966856994\t0.7809680065627563\t0.7629310344827587\t0.8388625592417062\t0.7990970654627539\n",
      "116\t1242.1391063155606\t20.152648588176817\t0.7697634105969191\t0.7735849056603774\t0.7402422611036339\t0.8688783570300158\t0.7994186046511628\n",
      "117\t1252.7033419534564\t18.33166579925455\t0.782912508289795\t0.7850697292863003\t0.768451519536903\t0.8388625592417062\t0.8021148036253778\n",
      "118\t1266.1842331960797\t19.997948414180428\t0.776720098776615\t0.77850697292863\t0.7673048600883653\t0.8230647709320695\t0.7942073170731706\n",
      "119\t1280.3361332118511\t17.80156174255535\t0.7902115717451433\t0.7924528301886793\t0.7737752161383286\t0.8483412322274881\t0.8093443858327054\n",
      "120\t1292.0369637543336\t17.71733530703932\t0.788695145819517\t0.7908121410992617\t0.7739130434782608\t0.8436018957345972\t0.8072562358276645\n",
      "121\t1305.7782922191545\t20.757517214864492\t0.7780464659862296\t0.7801476620180475\t0.7648766328011611\t0.8325434439178515\t0.7972768532526475\n",
      "122\t1317.9867140846327\t17.920241532381624\t0.7794658406526158\t0.782608695652174\t0.7548476454293629\t0.8609794628751974\t0.8044280442804429\n",
      "123\t1329.396833988838\t17.51447245851159\t0.7768131062333867\t0.7793273174733388\t0.7592592592592593\t0.8420221169036335\t0.798501872659176\n",
      "124\t1339.1951093813404\t18.803163079544902\t0.7843022284047468\t0.7867104183757178\t0.7668097281831188\t0.8467614533965245\t0.8048048048048049\n",
      "125\t1349.834650536999\t15.932862383313477\t0.7662318231079048\t0.7670221493027072\t0.7697063369397218\t0.7867298578199052\t0.778125\n",
      "126\t1361.9196882313117\t18.165202107280493\t0.7902412262965779\t0.793273174733388\t0.7649513212795549\t0.8688783570300158\t0.8136094674556213\n",
      "127\t1372.6443080445752\t15.654869912657887\t0.767900565593172\t0.771123872026251\t0.744475138121547\t0.8515007898894155\t0.7943994104642594\n",
      "128\t1383.9926631720737\t17.511852908413857\t0.7842388754994096\t0.7867104183757178\t0.7660485021398002\t0.8483412322274881\t0.8050974512743627\n",
      "129\t1396.025719829835\t15.764782106969506\t0.7744137834355068\t0.7760459392945037\t0.7670623145400594\t0.8167456556082149\t0.791124713083397\n",
      "130\t1406.9948488194495\t18.239379168953747\t0.7764963417067003\t0.7793273174733388\t0.7556179775280899\t0.8499210110584519\t0.8\n",
      "131\t1419.2429114980623\t17.1151916035451\t0.7905242924693615\t0.7940935192780968\t0.7595108695652174\t0.8830963665086888\t0.8166544923301682\n",
      "132\t1428.2743730982766\t16.94489374710247\t0.7808892591214706\t0.7834290401968826\t0.7624466571834992\t0.8467614533965245\t0.8023952095808383\n",
      "133\t1439.5030722850934\t15.778121971525252\t0.7816198394340834\t0.782608695652174\t0.7813455657492355\t0.8072669826224329\t0.7940947940947941\n",
      "134\t1449.9383811317384\t17.12485348014161\t0.792547541637686\t0.7957342083675144\t0.7651933701657458\t0.8751974723538705\t0.8165070007369197\n",
      "135\t1459.875386858359\t15.015085995197296\t0.7884754325520706\t0.7899917965545529\t0.7809239940387481\t0.8278041074249605\t0.8036809815950919\n",
      "136\t1471.1692145969719\t16.766551371430978\t0.7829421628412296\t0.7858900738310091\t0.7597765363128491\t0.8593996840442338\t0.8065233506300963\n",
      "137\t1482.0558817647398\t15.850401644129306\t0.7783335759614814\t0.7793273174733388\t0.7782874617737003\t0.8041074249605056\t0.790986790986791\n",
      "138\t1492.8825549101457\t14.646564606577158\t0.7865451908405178\t0.7891714520098442\t0.7662889518413598\t0.8546603475513428\t0.8080657206870799\n",
      "139\t1503.0293438732624\t16.010269121732563\t0.7781435172454696\t0.7793273174733388\t0.7757575757575758\t0.8088467614533965\t0.7919566898685229\n",
      "140\t1514.0563284270465\t14.783936116262339\t0.7852188236309033\t0.7875307629204266\t0.7686781609195402\t0.8451816745655608\t0.8051166290443943\n",
      "141\t1523.8459063703194\t14.283823595382273\t0.7881883225768189\t0.7908121410992617\t0.7677053824362606\t0.8562401263823065\t0.8095593726661687\n",
      "142\t1532.4384775161743\t13.799084635684267\t0.792834651612938\t0.7949138638228056\t0.7779390420899854\t0.8467614533965245\t0.8108925869894101\n",
      "143\t1540.3147347504273\t17.732356916181743\t0.7892949765189872\t0.7916324856439705\t0.7718794835007173\t0.8499210110584519\t0.8090225563909774\n",
      "144\t1551.0609501563013\t15.233195227221586\t0.7734594460529792\t0.7776866283839212\t0.7394179894179894\t0.8830963665086888\t0.8048956083513319\n",
      "145\t1562.1757796686143\t14.450226431246847\t0.7855355881575896\t0.7875307629204266\t0.7725947521865889\t0.8372827804107424\t0.8036391205458681\n",
      "146\t1572.189596619457\t15.84815984359011\t0.7858820072357106\t0.7883511074651354\t0.7674750356633381\t0.8499210110584519\t0.8065967016491755\n",
      "147\t1584.0213799430057\t13.207140110433102\t0.7840784713348322\t0.7875307629204266\t0.755464480874317\t0.8736176935229067\t0.8102564102564103\n",
      "148\t1595.4397095460445\t15.42381700174883\t0.7737842981846023\t0.7744052502050861\t0.778816199376947\t0.7898894154818326\t0.7843137254901961\n",
      "149\t1607.59499906376\t13.990874903625809\t0.7857593452275043\t0.7867104183757178\t0.7856049004594181\t0.8104265402843602\t0.7978227060653188\n",
      "150\t1618.109091958031\t19.463242957834154\t0.7922644754649024\t0.7949138638228056\t0.7708628005657708\t0.8609794628751974\t0.8134328358208954\n",
      "151\t1627.921086717397\t13.55731675797142\t0.7935908426745171\t0.7965545529122231\t0.7684797768479776\t0.8704581358609794\t0.8162962962962962\n",
      "152\t1637.3136611795053\t13.269985670689493\t0.7865788891944206\t0.7883511074651354\t0.7761413843888071\t0.8325434439178515\t0.8033536585365855\n",
      "153\t1650.1001909524202\t11.754244538256899\t0.7907480495392761\t0.793273174733388\t0.7709815078236131\t0.8562401263823065\t0.811377245508982\n",
      "154\t1662.9764380529523\t16.481031463947147\t0.7877488960419262\t0.7891714520098442\t0.781437125748503\t0.8246445497630331\t0.8024596464258262\n",
      "155\t1673.3491022353992\t11.760847388301045\t0.783415287730025\t0.7867104183757178\t0.7565337001375516\t0.8688783570300158\t0.8088235294117647\n",
      "156\t1682.7120551131666\t11.91736574866809\t0.7836727431538425\t0.7850697292863003\t0.7781109445277361\t0.8199052132701422\t0.7984615384615384\n",
      "157\t1690.6321263266727\t14.982119306456298\t0.7837954051620487\t0.7867104183757178\t0.7608391608391608\t0.8593996840442338\t0.8071216617210683\n",
      "158\t1700.4107884606346\t16.11020375182852\t0.7872757711531307\t0.7883511074651354\t0.7853881278538812\t0.8151658767772512\t0.7999999999999999\n",
      "159\t1714.649827979505\t12.475803817505948\t0.7797232960764332\t0.7809680065627563\t0.7764350453172205\t0.8120063191153238\t0.7938223938223938\n",
      "160\t1723.8673879224807\t15.783190654590726\t0.7943807320899989\t0.7973748974569319\t0.7688022284122563\t0.8720379146919431\t0.8171724648408587\n",
      "161\t1736.8738660514355\t12.191474746679887\t0.7899581601237943\t0.7924528301886793\t0.7706552706552706\t0.8546603475513428\t0.8104868913857677\n",
      "162\t1746.6654816986993\t10.945007021247875\t0.7894850352349989\t0.7916324856439705\t0.7742402315484804\t0.8451816745655608\t0.8081570996978852\n",
      "163\t1757.040257703513\t11.559805851895362\t0.7848683607503141\t0.7883511074651354\t0.7557980900409277\t0.8751974723538705\t0.8111273792093704\n",
      "164\t1765.7871150840074\t14.618513287277892\t0.7872124182477933\t0.7883511074651354\t0.7845220030349014\t0.8167456556082149\t0.8003095975232198\n",
      "165\t1776.6153439534828\t12.309107900946401\t0.7738476510899396\t0.7744052502050861\t0.7796875\t0.7883096366508688\t0.7839748625294578\n",
      "166\t1786.4795762402937\t13.904200533521362\t0.7843022284047468\t0.7867104183757178\t0.7668097281831188\t0.8467614533965245\t0.8048048048048049\n",
      "167\t1798.8708649389446\t11.868579718051478\t0.7852188236309033\t0.7875307629204266\t0.7686781609195402\t0.8451816745655608\t0.8051166290443943\n",
      "168\t1812.5366389499977\t13.473805965390056\t0.7892356674161181\t0.7899917965545529\t0.7913446676970634\t0.8088467614533965\t0.8\n",
      "169\t1825.1752733886242\t14.395573529414833\t0.772521283880325\t0.7727645611156686\t0.7825396825396825\t0.7788309636650869\t0.7806809184481392\n",
      "170\t1838.200619961135\t11.276505591813475\t0.7868323008157697\t0.7883511074651354\t0.7794336810730254\t0.8262243285939969\t0.8021472392638036\n",
      "171\t1851.0345329670236\t11.83635615138337\t0.7794995390065186\t0.7817883511074651\t0.7640287769784173\t0.8388625592417062\t0.799698795180723\n",
      "172\t1863.592868251726\t16.75105335051194\t0.7663881834700139\t0.7678424938474159\t0.7619760479041916\t0.8041074249605056\t0.7824750192159877\n",
      "173\t1874.1626481600106\t11.533005729084834\t0.782912508289795\t0.7850697292863003\t0.768451519536903\t0.8388625592417062\t0.8021148036253778\n",
      "174\t1886.771176177077\t10.449527623830363\t0.7726736004399657\t0.7752255947497949\t0.7553342816500711\t0.8388625592417062\t0.7949101796407185\n",
      "175\t1899.738103381358\t12.732735255733132\t0.7633890299726638\t0.7637407711238721\t0.7725118483412322\t0.7725118483412322\t0.7725118483412322\n",
      "176\t1910.6198449963704\t10.315711505594663\t0.796433635809758\t0.7998359310910582\t0.7660738714090287\t0.8846761453396524\t0.8211143695014664\n",
      "177\t1923.8087937878445\t10.400579802226275\t0.7855652427090241\t0.7883511074651354\t0.7637130801687764\t0.8578199052132701\t0.8080357142857143\n",
      "178\t1937.7625218620524\t13.712430977262557\t0.786452183383746\t0.7883511074651354\t0.7745241581259151\t0.8357030015797788\t0.8039513677811551\n",
      "179\t1950.4192239539698\t12.6580131535884\t0.7751443637481198\t0.7752255947497949\t0.7872\t0.7772511848341233\t0.7821939586645469\n",
      "180\t1959.3098772279918\t25.05078146746382\t0.7912211744280715\t0.7940935192780968\t0.7675070028011205\t0.8657187993680885\t0.8136599851521901\n",
      "181\t1970.858674298972\t9.548368147399742\t0.780259773870566\t0.7817883511074651\t0.7734724292101341\t0.8199052132701422\t0.7960122699386503\n",
      "182\t1980.414203999564\t10.079208550974727\t0.7927712987076007\t0.7949138638228056\t0.7771345875542692\t0.8483412322274881\t0.811178247734139\n",
      "183\t1990.0243260962889\t9.343150777276605\t0.7937215922876599\t0.7949138638228056\t0.789712556732224\t0.8246445497630331\t0.8068006182380216\n",
      "184\t1999.7457405775785\t10.000162008102052\t0.793434482312408\t0.7957342083675144\t0.7758620689655172\t0.8530805687203792\t0.8126410835214447\n",
      "185\t2010.116338708438\t9.299559198552743\t0.7828828537383606\t0.7842493847415914\t0.7777777777777778\t0.8183254344391785\t0.7975365665896844\n",
      "186\t2020.5041125444695\t11.010685721528716\t0.790591689177167\t0.7924528301886793\t0.7785923753665689\t0.8388625592417062\t0.8076045627376425\n",
      "187\t2028.499804300256\t9.973079659568612\t0.7796558993686277\t0.782608695652174\t0.7569832402234636\t0.8562401263823065\t0.8035581912527797\n",
      "188\t2035.6030469182879\t10.735848541429732\t0.7909084537038535\t0.7924528301886793\t0.7827380952380952\t0.8309636650868878\t0.8061302681992337\n",
      "189\t2043.0520809106529\t9.553573568206048\t0.7800697151545539\t0.7817883511074651\t0.7710487444608567\t0.8246445497630331\t0.7969465648854961\n",
      "190\t2049.815267033875\t10.28729035356082\t0.7914112331440832\t0.7940935192780968\t0.769774011299435\t0.8609794628751974\t0.8128262490678598\n",
      "191\t2057.5591836376116\t8.249616827175487\t0.7767537971305177\t0.7776866283839212\t0.7776073619631901\t0.8009478672985783\t0.7891050583657587\n",
      "192\t2067.1767040761188\t9.533956717175897\t0.7921377696542279\t0.7949138638228056\t0.7693389592123769\t0.8641390205371248\t0.8139880952380953\n",
      "193\t2081.2232392104343\t9.540836607688107\t0.7768090624309184\t0.7809680065627563\t0.7427055702917772\t0.8846761453396524\t0.8074981975486663\n",
      "194\t2092.2308139540255\t14.027075390331447\t0.7749839595835423\t0.7760459392945037\t0.774390243902439\t0.8025276461295419\t0.7882079131109386\n",
      "195\t2103.0670639155433\t18.275822436262388\t0.7869253082725416\t0.7891714520098442\t0.770893371757925\t0.8451816745655608\t0.8063300678221553\n",
      "196\t2114.021862026304\t9.280707611702383\t0.791537938954758\t0.7940935192780968\t0.7713068181818182\t0.8578199052132701\t0.8122662677636501\n",
      "197\t2123.6783804558218\t8.979367607738823\t0.7852821765362406\t0.7875307629204266\t0.7694524495677233\t0.8436018957345972\t0.8048229088168801\n",
      "198\t2135.2319388156757\t8.21603023535863\t0.7840191622319632\t0.7858900738310091\t0.7727272727272727\t0.8325434439178515\t0.8015209125475286\n",
      "199\t2146.005897144787\t9.203787892765831\t0.7911281669712997\t0.793273174733388\t0.7756874095513748\t0.8467614533965245\t0.8096676737160121\n",
      "200\t2156.070634163916\t8.688618970001698\t0.7818395527015296\t0.7834290401968826\t0.774145616641902\t0.8230647709320695\t0.7978560490045941\n",
      "201\t2166.3949841419235\t11.313116356963292\t0.7884417341981678\t0.7908121410992617\t0.7707736389684814\t0.8499210110584519\t0.8084147257700977\n",
      "202\t2179.768064642325\t11.637849861988798\t0.7958041505588536\t0.7981952420016407\t0.776824034334764\t0.8578199052132701\t0.8153153153153154\n",
      "203\t2190.779227352701\t8.022904369514436\t0.7832926257218187\t0.7850697292863003\t0.7731958762886598\t0.8293838862559242\t0.8003048780487806\n",
      "204\t2202.898418002762\t14.680273070116527\t0.7803864796812405\t0.7817883511074651\t0.775112443778111\t0.8167456556082149\t0.7953846153846154\n",
      "205\t2216.2420121505857\t9.58572021103464\t0.7692983733130605\t0.7694831829368335\t0.7802547770700637\t0.7740916271721959\t0.7771609833465504\n",
      "206\t2229.3563279109076\t9.75429686717689\t0.7934007839585053\t0.7965545529122231\t0.7662517289073306\t0.8751974723538705\t0.8171091445427728\n",
      "207\t2241.6858121072873\t10.017794000450522\t0.7844585887668558\t0.7875307629204266\t0.7597222222222222\t0.8641390205371248\t0.8085735402808574\n",
      "208\t2250.704531338066\t8.043192077340791\t0.7912885711358771\t0.7924528301886793\t0.7878787878787878\t0.8214849921011058\t0.8043310131477185\n",
      "209\t2258.674981135875\t9.784908389730845\t0.7853455294415779\t0.7875307629204266\t0.7702312138728323\t0.8420221169036335\t0.8045283018867925\n",
      "210\t2269.6014352235943\t9.14507909747772\t0.7912845273334087\t0.7940935192780968\t0.7682584269662921\t0.8641390205371248\t0.8133828996282528\n",
      "211\t2280.482937439345\t10.287518961937167\t0.7917279976707698\t0.7940935192780968\t0.7736389684813754\t0.8530805687203792\t0.8114199849737039\n",
      "212\t2292.303414626047\t8.302943704999052\t0.7899285055723597\t0.7916324856439705\t0.7799113737075333\t0.8341232227488151\t0.8061068702290075\n",
      "213\t2302.4734928468242\t8.333130136248656\t0.7810793178374823\t0.7834290401968826\t0.7647058823529411\t0.8420221169036335\t0.8015037593984962\n",
      "214\t2311.671884178184\t7.058149054297246\t0.793434482312408\t0.7957342083675144\t0.7758620689655172\t0.8530805687203792\t0.8126410835214447\n",
      "215\t2319.091089897789\t12.277298815955874\t0.778113862694035\t0.77850697292863\t0.7858267716535433\t0.7883096366508688\t0.7870662460567822\n",
      "216\t2329.9959829514846\t9.099308462347835\t0.7880953151200469\t0.7899917965545529\t0.7759882869692533\t0.8372827804107424\t0.8054711246200608\n",
      "217\t2340.186788628809\t9.13093064748682\t0.7934978352177452\t0.7957342083675144\t0.776657060518732\t0.8515007898894155\t0.8123587038432555\n",
      "218\t2350.372490081005\t9.108446125756018\t0.7819959130636387\t0.7842493847415914\t0.7665706051873199\t0.8404423380726699\t0.80180859080633\n",
      "219\t2360.5560112381354\t10.824505053693429\t0.7883487267413962\t0.7899917965545529\t0.7792592592592592\t0.8309636650868878\t0.8042813455657492\n",
      "220\t2371.7840131632984\t7.477030450871098\t0.7909084537038535\t0.7924528301886793\t0.7827380952380952\t0.8309636650868878\t0.8061302681992337\n",
      "221\t2383.2745351623744\t7.422088197025005\t0.7985876345912255\t0.7998359310910582\t0.7933634992458521\t0.8309636650868878\t0.8117283950617283\n",
      "222\t2394.327363011427\t7.96135643334128\t0.7966573928796725\t0.7990155865463495\t0.7779369627507163\t0.8578199052132701\t0.8159278737791135\n",
      "223\t2405.3018849808723\t9.161918279482052\t0.7796262448171931\t0.7817883511074651\t0.7655571635311144\t0.8357030015797788\t0.7990936555891238\n",
      "224\t2414.927322935313\t7.773610483855009\t0.792078460551359\t0.793273174733388\t0.7881996974281392\t0.8230647709320695\t0.8052550231839259\n",
      "225\t2426.4713263595477\t14.485182112082839\t0.7954577314807327\t0.7973748974569319\t0.7821637426900585\t0.8451816745655608\t0.8124525436598329\n",
      "226\t2438.4080697605386\t6.1426130807594745\t0.7906806528314705\t0.7949138638228056\t0.7529722589167768\t0.9004739336492891\t0.8201438848920863\n",
      "227\t2448.0476103397086\t7.347417165990919\t0.7764707309577341\t0.7768662838392125\t0.784251968503937\t0.7867298578199052\t0.7854889589905363\n",
      "228\t2459.7543792221695\t10.090547699102899\t0.7805468838458179\t0.7809680065627563\t0.7877358490566038\t0.7914691943127962\t0.7895981087470448\n",
      "229\t2468.352508842945\t9.367906679777661\t0.7823463759442278\t0.7834290401968826\t0.7808219178082192\t0.8104265402843602\t0.7953488372093024\n",
      "230\t2480.9062305819243\t7.882250022958033\t0.7864818379351806\t0.7891714520098442\t0.7655367231638418\t0.8562401263823065\t0.8083519761372111\n",
      "231\t2491.534244487062\t8.372508516622474\t0.787182763696359\t0.7875307629204266\t0.7949526813880127\t0.7962085308056872\t0.7955801104972375\n",
      "232\t2502.9699533563107\t14.941776728373952\t0.7901185642883717\t0.7916324856439705\t0.7824143070044709\t0.8293838862559242\t0.8052147239263804\n",
      "233\t2513.6079200860113\t6.5679068501340225\t0.7923615267241425\t0.7940935192780968\t0.7817109144542773\t0.8372827804107424\t0.8085430968726163\n",
      "234\t2522.7914925143123\t6.164535856922157\t0.7824393834009996\t0.7842493847415914\t0.7720588235294118\t0.8293838862559242\t0.7996953541507997\n",
      "235\t2529.7639639070258\t9.694110431853915\t0.7903679321072523\t0.793273174733388\t0.7664335664335664\t0.8657187993680885\t0.8130563798219586\n",
      "236\t2538.792918282561\t6.655591137998272\t0.787495484420577\t0.7891714520098442\t0.7781065088757396\t0.8309636650868878\t0.8036669213139801\n",
      "237\t2547.9063450405374\t5.643881136260461\t0.7914745860494208\t0.7940935192780968\t0.7705382436260623\t0.8593996840442338\t0.8125466766243465\n",
      "238\t2558.3016013531014\t8.706203412919422\t0.7776623047517375\t0.7817883511074651\t0.7436918990703851\t0.8846761453396524\t0.8080808080808081\n",
      "239\t2566.550737924874\t9.321397678693756\t0.7849357574581197\t0.7867104183757178\t0.7746686303387335\t0.8309636650868878\t0.8018292682926829\n",
      "240\t2575.4253422506154\t15.385003553237766\t0.7900552113830344\t0.7916324856439705\t0.7815750371471025\t0.8309636650868878\t0.8055130168453293\n",
      "241\t2590.399088360369\t6.563265946868341\t0.7816791485369522\t0.7842493847415914\t0.7627840909090909\t0.8483412322274881\t0.8032909498878085\n",
      "242\t2602.4678362756968\t6.288400574907428\t0.7980471129946245\t0.800656275635767\t0.7762039660056658\t0.8657187993680885\t0.818521284540702\n",
      "243\t2612.8172690290958\t6.254980218276614\t0.782443427203468\t0.782608695652174\t0.7929936305732485\t0.7867298578199052\t0.7898493259318002\n",
      "244\t2620.294230811298\t9.32479618594516\t0.7947905040734571\t0.7981952420016407\t0.7647058823529411\t0.8830963665086888\t0.8196480938416422\n",
      "245\t2629.2276493655518\t7.8236870904802345\t0.8013967293725636\t0.8039376538146021\t0.7798295454545454\t0.8672985781990521\t0.8212415856394913\n",
      "246\t2641.1607596576214\t13.779268471291289\t0.7967207457850098\t0.7990155865463495\t0.7787356321839081\t0.8562401263823065\t0.8156508653122649\n",
      "247\t2652.7914026640356\t6.444713135482743\t0.7888852045355288\t0.7908121410992617\t0.7763157894736842\t0.8388625592417062\t0.8063781321184511\n",
      "248\t2663.785843585618\t7.018533720343839\t0.7868026462643353\t0.7875307629204266\t0.7894736842105263\t0.8056872037914692\t0.7974980453479281\n",
      "249\t2674.7534344969317\t6.380708486161893\t0.7927376003536979\t0.7957342083675144\t0.7674094707520891\t0.8704581358609794\t0.8156920799407845\n",
      "250\t2684.0939289825037\t6.010052393088699\t0.7900848659344688\t0.7924528301886793\t0.7722063037249284\t0.8515007898894155\t0.8099173553719008\n",
      "251\t2694.2046436788514\t5.986727239927859\t0.7950776140487089\t0.7973748974569319\t0.7772988505747126\t0.8546603475513428\t0.8141459744168548\n",
      "252\t2701.793181319721\t5.5001019449555315\t0.7856326394168297\t0.7867104183757178\t0.7838660578386606\t0.8135860979462876\t0.7984496124031008\n",
      "253\t2709.2847261428833\t16.782418474947917\t0.7962476208962145\t0.7981952420016407\t0.7824817518248175\t0.8467614533965245\t0.8133535660091048\n",
      "254\t2718.9475608430803\t6.0993651012540795\t0.790591689177167\t0.7924528301886793\t0.7785923753665689\t0.8388625592417062\t0.8076045627376425\n",
      "255\t2729.245407207869\t6.320911354501732\t0.7951409669540461\t0.7973748974569319\t0.7780979827089337\t0.8530805687203792\t0.8138658628485306\n",
      "256\t2738.9527504649013\t5.3396979590761475\t0.788851506181626\t0.7916324856439705\t0.7665260196905767\t0.8609794628751974\t0.8110119047619047\n",
      "257\t2747.783154037781\t7.355535202572355\t0.7799767076977824\t0.7809680065627563\t0.7798165137614679\t0.8056872037914692\t0.7925407925407925\n",
      "258\t2756.7531503634527\t7.280649185297079\t0.7969067606985534\t0.800656275635767\t0.7628032345013477\t0.8941548183254344\t0.8232727272727274\n",
      "259\t2767.6691197995096\t5.979866699781269\t0.7930880632342872\t0.7949138638228056\t0.7812041116005873\t0.8404423380726699\t0.8097412480974124\n",
      "260\t2778.696591197513\t9.454149367229547\t0.7755837902830123\t0.7768662838392125\t0.7722473604826546\t0.8088467614533965\t0.7901234567901234\n",
      "261\t2789.3426796374843\t7.667828205332626\t0.7847456987421079\t0.7867104183757178\t0.7722627737226277\t0.8357030015797788\t0.802731411229135\n",
      "262\t2800.1759213469923\t6.843464903882705\t0.7853455294415779\t0.7875307629204266\t0.7702312138728323\t0.8420221169036335\t0.8045283018867925\n",
      "263\t2810.9077652730048\t4.827925598074216\t0.7821522734257478\t0.7850697292863003\t0.7594405594405594\t0.8578199052132701\t0.8056379821958457\n",
      "264\t2822.584685187787\t9.317915015039034\t0.7975106352004918\t0.7998359310910582\t0.7790530846484935\t0.8578199052132701\t0.8165413533834587\n",
      "265\t2835.175021455623\t6.253437580104219\t0.7910014611606252\t0.793273174733388\t0.7741007194244605\t0.8499210110584519\t0.8102409638554218\n",
      "266\t2847.0444622039795\t11.710056609474123\t0.7884080358442651\t0.7916324856439705\t0.7613793103448275\t0.8720379146919431\t0.812960235640648\n",
      "267\t2856.5749563435093\t5.586087254487211\t0.7898017997616853\t0.7916324856439705\t0.7782672540381792\t0.8372827804107424\t0.806697108066971\n",
      "268\t2866.1601566439494\t5.196678208070807\t0.7923911812755771\t0.7949138638228056\t0.7724039829302988\t0.8578199052132701\t0.812874251497006\n",
      "269\t2875.9884227104485\t10.48930436585215\t0.7790560686691577\t0.7817883511074651\t0.7588152327221439\t0.8499210110584519\t0.8017883755588674\n",
      "270\t2883.523103101179\t8.206577589327935\t0.7905283362718297\t0.7924528301886793\t0.7777777777777778\t0.8404423380726699\t0.8078967350037966\n",
      "271\t2892.2134210839868\t5.471427457057871\t0.788538785457408\t0.7899917965545529\t0.7817638266068759\t0.8262243285939969\t0.8033794162826421\n",
      "272\t2901.0060273343697\t5.144181050680345\t0.7874658298691426\t0.7883511074651354\t0.7880184331797235\t0.8104265402843602\t0.7990654205607478\n",
      "273\t2910.804562108591\t8.27646759871277\t0.7894553806835642\t0.7908121410992617\t0.7837837837837838\t0.8246445497630331\t0.8036951501154735\n",
      "274\t2920.3469280125573\t5.64433718707005\t0.7915716373086608\t0.793273174733388\t0.7813884785819794\t0.8357030015797788\t0.8076335877862596\n",
      "275\t2932.4516345076263\t5.881476902985014\t0.7896750939510107\t0.7916324856439705\t0.7766423357664234\t0.8404423380726699\t0.8072837632776936\n",
      "276\t2944.698637953028\t6.942307658464415\t0.8013670748211292\t0.8031173092698933\t0.7893961708394698\t0.8467614533965245\t0.8170731707317073\n",
      "277\t2959.7547897147015\t5.510597842119751\t0.777379238578954\t0.7809680065627563\t0.748641304347826\t0.8704581358609794\t0.8049671292914535\n",
      "278\t2972.7772167511284\t13.379529009340331\t0.7976036426572634\t0.800656275635767\t0.7708333333333334\t0.8767772511848341\t0.8203991130820398\n",
      "279\t2982.30489022471\t7.967930220824201\t0.7892316236136496\t0.7916324856439705\t0.771101573676681\t0.8515007898894155\t0.8093093093093093\n",
      "280\t2993.444254193455\t5.611296984861838\t0.7912548727819744\t0.793273174733388\t0.777292576419214\t0.8436018957345972\t0.8090909090909091\n",
      "281\t3004.7916181376204\t5.4940044903196394\t0.7944777833492389\t0.7965545529122231\t0.7793904208998549\t0.8483412322274881\t0.8124054462934946\n",
      "282\t3015.6766426889226\t5.813292060935055\t0.7962476208962145\t0.7981952420016407\t0.7824817518248175\t0.8467614533965245\t0.8133535660091048\n",
      "283\t3026.315998967737\t6.302900499431416\t0.796184267990877\t0.7981952420016407\t0.7816593886462883\t0.8483412322274881\t0.8136363636363637\n",
      "284\t3032.7702314695343\t13.132806028675986\t0.7740633205549177\t0.7768662838392125\t0.7538677918424754\t0.8467614533965245\t0.7976190476190477\n",
      "285\t3040.3766686199233\t7.410000492207473\t0.7918547034814445\t0.7940935192780968\t0.7752161383285303\t0.8499210110584519\t0.8108515448379805\n",
      "286\t3054.3793069366366\t4.206738015695009\t0.7990270611261181\t0.8014766201804758\t0.7788873038516405\t0.8625592417061612\t0.8185907046476761\n",
      "287\t3068.350721665658\t4.424975109330262\t0.7861354188570597\t0.7883511074651354\t0.7705627705627706\t0.8436018957345972\t0.8054298642533937\n",
      "288\t3080.2082722336054\t6.504591267585056\t0.7989003553154436\t0.8014766201804758\t0.7773049645390071\t0.8657187993680885\t0.8191330343796711\n",
      "289\t3090.4396788235754\t6.002741462842096\t0.7896117410456733\t0.7916324856439705\t0.7758369723435226\t0.8420221169036335\t0.8075757575757575\n",
      "290\t3103.4376400168985\t5.824965375766624\t0.7876814993341206\t0.7908121410992617\t0.7617728531855956\t0.8688783570300158\t0.8118081180811809\n",
      "291\t3114.0027077393606\t4.931743293986074\t0.7973205764844798\t0.7998359310910582\t0.7766714082503556\t0.8625592417061612\t0.8173652694610779\n",
      "292\t3124.056689908728\t13.079089858627412\t0.7647490955361812\t0.7645611156685808\t0.7808441558441559\t0.7598736176935229\t0.7702161729383509\n",
      "293\t3134.563991971314\t6.8060106797202025\t0.7893246310704214\t0.7924528301886793\t0.7631578947368421\t0.8704581358609794\t0.8132841328413284\n",
      "294\t3145.77216718439\t4.549348100626958\t0.7966573928796725\t0.7990155865463495\t0.7779369627507163\t0.8578199052132701\t0.8159278737791135\n",
      "295\t3157.0045592254028\t4.3945851658136235\t0.792234820913468\t0.7940935192780968\t0.7800586510263929\t0.8404423380726699\t0.809125475285171\n",
      "296\t3168.2390810968354\t3.855969672687934\t0.791537938954758\t0.7940935192780968\t0.7713068181818182\t0.8578199052132701\t0.8122662677636501\n",
      "297\t3179.864567265846\t11.331547957874136\t0.7907480495392761\t0.793273174733388\t0.7709815078236131\t0.8562401263823065\t0.811377245508982\n",
      "298\t3191.565226452425\t6.3415643500193255\t0.7911281669712997\t0.793273174733388\t0.7756874095513748\t0.8467614533965245\t0.8096676737160121\n",
      "299\t3203.201399982907\t7.193855085992254\t0.7889782119923007\t0.7916324856439705\t0.768033946251768\t0.8578199052132701\t0.8104477611940297\n",
      "300\t3216.3872112240642\t5.381665498163784\t0.7939413055551062\t0.7957342083675144\t0.7823529411764706\t0.8404423380726699\t0.8103579588728105\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # (task, dataset, radius, dim, layer_hidden, layer_output,\n",
    "    #  batch_train, batch_test, lr, lr_decay, decay_interval, iteration,\n",
    "    #  setting) = sys.argv[1:]\n",
    "    # (radius, dim, layer_hidden, layer_output,\n",
    "    #  batch_train, batch_test, decay_interval,\n",
    "    #  iteration) = map(int, [radius, dim, layer_hidden, layer_output,\n",
    "    #                         batch_train, batch_test,\n",
    "    #                         decay_interval, iteration])\n",
    "    # lr, lr_decay = map(float, [lr, lr_decay])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print('The code uses a GPU!')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print('The code uses a CPU...')\n",
    "    print('-'*100)\n",
    "\n",
    "    print('Preprocessing the', dataset, 'dataset.')\n",
    "    print('Just a moment......')\n",
    "    (dataset_train, dataset_dev, dataset_test, N_fingerprints) = create_datasets(task, dataset, radius, device)\n",
    "    print('-'*100)\n",
    "\n",
    "    print('The preprocess has finished!')\n",
    "    print('# of training data samples:', len(dataset_train))\n",
    "    print('# of development data samples:', len(dataset_dev))\n",
    "    print('# of test data samples:', len(dataset_test))\n",
    "    print('-'*100)\n",
    "\n",
    "    print('Creating a model.')\n",
    "    torch.manual_seed(1234)\n",
    "    model = MolecularGraphNeuralNetwork(\n",
    "            N_fingerprints, dim, layer_hidden, layer_output).to(device)\n",
    "    trainer = Trainer(model)\n",
    "    tester = Tester(model)\n",
    "    print('# of model parameters:',\n",
    "          sum([np.prod(p.size()) for p in model.parameters()]))\n",
    "    print('-'*100)\n",
    "\n",
    "#     file_result = '../dataset/classification--' + setting + '.txt'\n",
    "#     if task == 'classification':\n",
    "#         result = 'Epoch\\tTime(sec)\\tLoss_train\\tAUC_dev\\tAUC_test'\n",
    "#     if task == 'regression':\n",
    "#         result = 'Epoch\\tTime(sec)\\tLoss_train\\tMAE_dev\\tMAE_test'\n",
    "\n",
    "#     with open(file_result, 'w') as f:\n",
    "#         f.write(result + '\\n')\n",
    "\n",
    "#     print('Start training.')\n",
    "#     print('The result is saved in the output directory every epoch!')\n",
    "\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    for epoch in range(iteration):\n",
    "\n",
    "        epoch += 1\n",
    "        if epoch % decay_interval == 0:\n",
    "            trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "        loss_train = trainer.train(dataset_train)\n",
    "\n",
    "        if task == 'classification':\n",
    "            # prediction_dev = tester.test_classifier(dataset_dev)\n",
    "            AUC, acc, precision, recall, f1, aupr = tester.test_classifier(dataset_test)\n",
    "        if task == 'regression':\n",
    "            prediction_dev = tester.test_regressor(dataset_dev)\n",
    "            prediction_test = tester.test_regressor(dataset_test)\n",
    "\n",
    "        time = timeit.default_timer() - start\n",
    "\n",
    "        if epoch == 1:\n",
    "            minutes = time * iteration / 60\n",
    "            hours = int(minutes / 60)\n",
    "            minutes = int(minutes - 60 * hours)\n",
    "            print('The training will finish in about',\n",
    "                  hours, 'hours', minutes, 'minutes.')\n",
    "            print('-'*100)\n",
    "        # print('time, loss_train, prediction_dev, prediction_test')\n",
    "        result = '\\t'.join(map(str, [epoch, time, loss_train,\n",
    "                                      AUC, acc, precision, recall, f1]))\n",
    "        # tester.save_result(result, file_result)\n",
    "\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0a740fe-80d9-4c93-b4a8-4e1da43b8cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7939413055551062,\n",
       " 0.7957342083675144,\n",
       " 0.7823529411764706,\n",
       " 0.8404423380726699,\n",
       " 0.8103579588728105,\n",
       " 0.8528250391323634)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC,acc,precision,recall,f1, aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d64e0-9ebd-4e6d-8217-bc190609675b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311d1f8-cddb-4092-8cf0-8d810afff5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37aa0b-3bf0-483c-88ec-f5dcb7e3be8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55953064-5682-47e9-8a33-98191e80593f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd02e4-e7f4-4e40-8dfa-08894226acf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
